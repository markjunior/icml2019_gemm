\subsection{Related Work}

As there are multiple AI frameworks and a wide range of hardware involved in deep learning applications nowadays, it is important but challenging for compiler-level optimization to efficiently and flexibly harmonize AI algorithms with the underlying hardware and optimize the performance of various deep learning applications. Much work has explored this space and achieve good performance improvement. CuDNN \cite{chetlur2014cudnn} provides highly efficient implementations of various deep neural network layers and it is considered the standard for accelerating deep learning on Nvidia GPUs. NNPACK \cite{dukhan2016nnpack} and PCL-DNN \cite{das2016distributed} similarly provide accelerations in x86 processors. Latte \cite{truong2016latte} provides a natural abstraction for specifying new layers and applies domain-specific and general computation graph optimization. XLA (Accelerated Linear Algebra) \cite{leary2017xla} optimizes TensorFlow computations in terms of speed, memory usage, and portability via just-in-time (JIT) compilation or ahead-of-time (AOT) compilation.

However, the aforementioned approaches perform either graph-level or operator-level optimization during compilation and they are not generic enough to accommodate all AI frameworks and hardware. Based on the structure of Halide \cite{ragan2013halide}, \cite{Chen18} proposes a general end-to-end compilation optimization framework combining Neural Network Virtual Machine (NNVM) \cite{nnvm2017} for computation graphs and Tensor Virtual Machine (TVM) \cite{Chen18} for tensor operators. 
%For tensor operators, e.g., GEMM, the configuration tuning efficiency affects the performance for general deep learning. 
Currently in TVM, a tuning method based on XGBoost \cite{chen2016xgboost} is considered the state-of-the-art method for GEMM configuration optimization and has shown superior performance over other methods \cite{Chen18}. %while great potential still exists based on the current development configuration optimization techniques.

For configuration optimization, the intuitive method is grid search, where all possible configuration candidates are tested sequentially so as to find the configuration with best performance. It guarantees to find the global optimal configurations, but the number of tested configuration candidates grows exponentially with the dimensions of configuration space \cite{bellman2015adaptive}, so its usage is limited in problems with small search space or in combination with manual search \cite{hinton2012practical,lecun2012efficient,larochelle2007empirical}. Random search is proposed where the configurations are randomly selected to be tested, and is shown empirically and theoretically to be more efficient than grid search for configuration tuning \cite{bergstra2012random}.
%has  shown that random search is 

As an instance of Bayesian optimization, sequential model-based optimization (SMBO) shows its strength in configuration tuning by iteratively updating the underlying expected improvement, exploring new data through an acquisition function, and training a regression model \cite{hutter2011sequential,bergstra2011algorithms,hoffman2014modular}. The general method has been widely adopted and implemented  \cite{kandasamy2018neural,snoek2012practical}.

%, and a large number of configuration optimization software libraries including Hyperopt \cite{bergstra2013hyperopt}, Spearmint \cite{snoek2012practical}, GPyOpt \cite{gpyopt2016}, MOE \cite{clark2014moe}, RoBO \cite{klein-bayesopt17} and SMAC3 \cite{smac-2017} are widely implemented in academic and industrial areas.

From another perspective, a series of evolutionary approaches have been explored, including the broad class of genetic algorithms (GA) \cite{Holland1975,Goldberg1989}, differential evolution \cite{Storn1997}, estimation of distribution algorithms \cite{larranaga2001estimation,bosman2007adapted} and particle swarm optimization \cite{kennedy2001swarm}.  Evolutionary strategies (ES) \cite{rechenberg1994evolutionsstrategie,schwefel1977numerische} have shown to perform efficiently in configuration tuning. Based on the concept, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), first proposed by \cite{hansen2001completely}, has shown excellent performance by smartly update the mean, step size and covariance matrix for each evolution \cite{LoshchilovH16} . Natural Evolution Strategies \cite{wierstra2014natural} applies natural gradients to update configuration search policy so as to achieve high expected fitness and discover the high-performance configuration.

% In general EA, for every searching iterations of EA, a population of configurations are firstly generated through mutation. After evaluating the fitness of the candidates, the highest scoring configuration candidates are preserved and recombined to form the new population for the next generation. The iteration continues until the optimal configuration candidates are discovered and converged \cite{salimans2017evolution}.

Recently, researchers at Google apply deep reinforcement learning with a RNN controller to optimize the configurations of neural network architectures and its components \cite{bello2017neural,mirhoseini2017device,ramachandran2018searching,zoph2016neural,pham2018efficient}. The excellent tuning performance in wide range of applications shows the potential of deep reinforcement learning in the configuration tuning area. 
